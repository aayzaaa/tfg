import logging
import os
import sys
from collections import deque
from pickle import Pickler, Unpickler
from random import shuffle
import concurrent.futures
from concurrent.futures._base import as_completed
import time

import numpy as np
from tqdm import tqdm

from Match import Match
from MonteCarloTreeSearch import MonteCarloTreeSearch
from Caching import Caching

log = logging.getLogger(__name__)


class Training:
    """
    Handles the self-play sessions, where the neural network plays against itself,
        and the learning sessions, where the neural networks learns from the previous games.
    """

    def __init__(self, game, neural_network, args):
        self.game = game
        self.neural_network = neural_network
        # Create a rival neural network to play against
        self.rival_network = self.neural_network.__class__(self.game)
        self.args = args
        self.monte_carlo_tree_search = MonteCarloTreeSearch(self.game, self.neural_network, self.args)
        # Keeps a copy of all the training examples
        self.train_data_history = []
        # Used to skip the first self play (learn before play)
        self.skip_first_self_play = False

    def self_play(self, caching):
        """ Play the game one time against itself, starting with player 1.
            As the game progresses, each turn is added as training data.
            Once the game is over, the outcome of the game is assigned at the end
            of each of the training instances.

            While moves_performed < tempThreshold employs a temp=1 (exploration),
                and after that it goes to temp=0 (best move).

        :returns: a list of training data with the format:
                    (canonical_board, current_player, policy, value)
        """
        training_examples = []
        board = self.game.get_initial_board()
        self.current_player = 1
        moves_performed = 0

        # Create a MCTS
        monte_carlo_tree_search = MonteCarloTreeSearch(self.game, self.neural_network, self.args, caching=caching)

        # Play until the game is over
        while True:
            moves_performed += 1
            canonical_board = self.game.get_canonical_form(board, self.current_player)
            temp = int(moves_performed < self.args.tempThreshold)

            if moves_performed <= 4:
                active_caching = True
            else:
                active_caching = False

            # Get the move probabilities
            probabilities = monte_carlo_tree_search.get_action_probabilities(canonical_board, temp=temp, active_caching=active_caching)

            # The board can have symmetry, so here we retrieve all mirrored positions...
            symmetries = self.game.get_symmetries(canonical_board, probabilities)
            for board_symm, probabilities_symm in symmetries:
                # ... and append it as training data.
                training_examples.append([board_symm, self.current_player, probabilities_symm, None])

            # Randomly choose an action based on the probabilities and make the move
            action = np.random.choice(len(probabilities), p=probabilities)
            board, self.current_player = self.game.get_next_state(board, self.current_player, action)

            # Check if the game is over
            result = self.game.get_game_ended(board, self.current_player)

            if result != 0:
                # If it's over add the result to each training data and return them
                return [(x[0], x[2], result * ((-1) ** (x[1] != self.current_player))) for x in training_examples]

    def learn_full_alpha(self):
        """ Execute a number of training iterations with a number of self-play games
            in each iteration.
            After the number of self-play games in one iteration is done, the training
            data generated by the self-play games is used to train retrain the neural network.
            Then the new trained neural network is put against the previous version to
            see which one is better. Finally we keep this version and start a new iteration.
        """
        # ITERATION
        for iteration in range(self.args.starting_iteration, self.args.numIters + 1):

            if iteration == 1:
                # Store the initial network
                self.neural_network.save_checkpoint(folder=self.args.checkpoint,
                                                    filename=f'model_iteration_0.pth.tar')

            print(f"MaxItersHistory: {self.args.numItersForTrainExamplesHistory}")
            # bookkeeping
            log.info(f'Starting Iter #{iteration} ...')

            # Generate training data by self-playing
            if not self.skip_first_self_play or iteration > self.args.starting_iteration:
                iteration_training_data = deque([], maxlen=self.args.maxlenOfQueue)

                # SELF-PLAY GAMES
                # Execute self-play games in multithreading mode in order to speed up the process.
                start_time = time.time()

                caching = Caching()
                for _ in tqdm(range(self.args.numEps), desc="Self Play"):
                    iteration_training_data += self.self_play(caching)

                final_time = time.time() - start_time
                log.info('SELF-PLAY TIME: %d:%d:%d' % (final_time//3600, final_time % 3600 // 60, final_time % 60 // 1))

                # Save the training data to the history list
                self.train_data_history.append(iteration_training_data)

            # Make sure that the training data history doesn't grow too large
            if iteration <= 5:
                self.args.numItersForTrainExamplesHistory = 4
            else:
                self.args.numItersForTrainExamplesHistory = 4 + int((iteration - 4) / 2)
                if self.args.numItersForTrainExamplesHistory > 20:
                    self.args.numItersForTrainExamplesHistory = 20

            if len(self.train_data_history) > self.args.numItersForTrainExamplesHistory:
                log.warning(
                    f"Removing the oldest entry in training_data_history. len(trainExamplesHistory) = {len(self.train_data_history)}")
                self.train_data_history.pop(0)
            # Story history to a file
            self.save_training_data(iteration - 1)

            # Shuffle all the data before training to improve performance
            training_data = []
            for example in self.train_data_history:
                training_data.extend(example)
            shuffle(training_data)

            # TRAINING
            # Train the neural network with the training data
            self.neural_network.train(training_data)
            neural_network_monte_carlo_tree_search = MonteCarloTreeSearch(self.game, self.neural_network, self.args, noise=False)
            self.neural_network.save_checkpoint(folder=self.args.checkpoint,
                                                filename=f'model_iteration_{iteration}.pth.tar')

    def get_checkpoint_file(self, iteration):
        """ Returns the string representing the checkpoint neural network.

        :param iteration: Number of the current iteration.
        :returns: String representing the file of the current checkpoint neural network.
        """
        return 'checkpoint_' + str(iteration) + '.pth.tar'

    def save_training_data(self, iteration):
        """ Save the current training data.

        :param iteration: Number of the current iteration.
        """
        folder = self.args.checkpoint
        if not os.path.exists(folder):
            os.makedirs(folder)
        filename = os.path.join(folder, self.get_checkpoint_file(iteration) + ".training_data")
        with open(filename, "wb+") as f:
            Pickler(f).dump(self.train_data_history)
        f.closed
        log.info('Training data stored: ' + filename + '.')

    def load_training_data(self):
        """ Load stored training data.
        """
        modelFile = os.path.join(self.args.load_folder_file_training_data[0],
                                 self.args.load_folder_file_training_data[1])
        examplesFile = modelFile + ".training_data"
        if not os.path.isfile(examplesFile):
            log.warning(f'File "{examplesFile}" with trainExamples not found!')
            r = input("Continue? [y|n]")
            if r != "y":
                sys.exit()
        else:
            log.info("File with trainExamples found. Loading it...")
            with open(examplesFile, "rb") as f:
                self.train_data_history = Unpickler(f).load()
            log.info('Loading done!')

            # examples based on the model were already collected (loaded)
            self.skip_first_self_play = True
